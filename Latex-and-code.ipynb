{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $$ \\text{Baye's Theorem } $$\n",
    "\n",
    "\n",
    "\n",
    "### $$P(y|x) = \\frac{P(x|y)P(y)}{P(x)}$$\n",
    "\n",
    "#### $$\\text{where:}$$\n",
    "\n",
    "\\begin{align*}\n",
    "P(y) & : \\text{prior probability of event y} \\\\\n",
    "P(y|x) & : \\text{probability of event y given event x} \\\\\n",
    "P(x|y) & : \\text{probability of event x given event y} \\\\\n",
    "P(x) & : \\text{prior probability of event x}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "##### $$\\text{ Assume that features are mutually independent (Naive assumption)}$$\n",
    "\n",
    "##### $$\\text{so we get in our case}$$\n",
    "\n",
    "#### $$P(y|\\mathbf{X}) = \\frac{\\prod_{i=1}^{n} P(x_i|y)P(y)}{P(\\mathbf{X})}$$\n",
    "\n",
    "##### $$\\text{where:}$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{X} & : \\text{vector of features } (x_1, x_2, \\ldots, x_n) \\\\\n",
    "x_i & : \\text{individual feature} \\\\\n",
    "n & : \\text{number of features} \\\\\n",
    "y & : \\text{event that the person would live in the next 10 years} \\\\\n",
    "P(y) & : \\text{prior probability of event } y \\\\\n",
    "P(y|\\mathbf{X}) & : \\text{probability of event } y \\text{ given features } \\mathbf{X} \\text{  (Posterior) } \\\\\n",
    "P(\\mathbf{X}|y) & : \\text{probability of features } \\mathbf{X} \\text{ given event } y \\text{  (LikeHood) } \\\\\n",
    "P(\\mathbf{X}) & : \\text{prior probability of features } \\mathbf{X}\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "##### $$\\text{Then to select class with highest posterior probability }$$\n",
    "\n",
    "### $$y = \\arg\\max_y P(y|\\mathbf{X}) = \\arg\\max_y \\frac{\\prod_{i=1}^{n} P(x_i|y)P(y)}{P(\\mathbf{X})}$$\n",
    "\n",
    "##### $$ \\text{Since } P(\\mathbf{X}) \\text{does not depend on our posterior probability at all we can neglect it} $$\n",
    "\n",
    "### $$\\arg\\max_y P(y|\\mathbf{X}) = \\arg\\max_y \\left( \\prod_{i=1}^{n} P(x_i|y)P(y) \\right)$$\n",
    "\n",
    "##### $$\\text{As the values of the probabilities is between 0 and 1,}$$\n",
    "##### $$\\text{so multiplying them would result in a very small number,}$$\n",
    "##### $$\\text{so we would apply log function to allow us to change the multiplicatoin to summation as follows :}$$\n",
    "\n",
    "## $$y = \\arg\\max_y \\left( \\sum_{i=1}^{n} \\log(P(x_i|y)) \\right) + \\log(P(y))$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### $$\\text{Finally, We need to calculate the following}$$\n",
    "\n",
    "##### $$\\text{P(y)    Prior probability  -> Frequency of each class} $$\n",
    "\n",
    "##### $$P(\\mathbf{x_i}|y) \\text{     class conditional probability  -> Model with Gaussian}$$\n",
    "\n",
    "## $$P(x_{i}\\mid y) = \\frac{1}{\\sqrt{2\\pi \\sigma_y^{2}}} \\exp \\left(-\\frac{(x_{i} -\\mu_{y})^2}{2\\sigma_y^{2}} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "### Training\n",
    "- Calculate mean,var and prior(frequency) for each class\n",
    "\n",
    "### Predictions\n",
    "- Calculate posterior for each class\n",
    "- Choose class with highest posterior probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self,X,y):\n",
    "        n_samples , n_features = X.shape\n",
    "        self._classes = np.unique(y) # 0 or 1\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        self._mean = np.zeros((n_classes , n_features) , dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes , n_features) , dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes  , dtype=np.float64)\n",
    "        for i , c in enumerate(self._classes):\n",
    "            X_c = X[y == c]\n",
    "            self._mean[i, :] = X_c.mean(axis = 0) #mean of each feature in each class\n",
    "            self._var[i,:] = X_c.var(axis = 0)\n",
    "            self._priors[i]=X_c.shape[0] / float(n_samples) #prob of each class\n",
    "    def predict(self,X):\n",
    "        Z=np.array(X)\n",
    "        y_pred = [self._predict(x) for x in Z]\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def _predict(self,x):\n",
    "            posteriors = []\n",
    "            for i , c in enumerate(self._classes): #calculate posterior for each class\n",
    "                prior = np.log(self._priors[i])\n",
    "                \n",
    "                posterior = np.sum(np.log(self._pdf(i,x))) #gaussian model \n",
    "                posterior += prior\n",
    "                posteriors.append(posterior)\n",
    "            return self._classes[np.argmax(posteriors)]\n",
    "\n",
    "    def _pdf(self, i , x):\n",
    "        mean = self._mean[i]\n",
    "        var = self._var[i]\n",
    "        \n",
    "        numerator  = np.exp(-(((x - mean) **2) / (2 * var)))\n",
    "        doneminator = np.sqrt(2 *np.pi * var)\n",
    "        return numerator / doneminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(a,b):\n",
    "    return np.sum(a == b) / len( a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7613741875580315"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "NB=NaiveBayes()\n",
    "NB.fit(X_train,y_train)\n",
    "y_pred = NB.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
